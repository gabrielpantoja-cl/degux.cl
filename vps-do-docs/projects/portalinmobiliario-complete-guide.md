# üè† Gu√≠a Completa: Web Scraping Portal Inmobiliario con n8n

## üìñ Tabla de Contenidos

1. [Introducci√≥n y Fundamentos Te√≥ricos](#1-introducci√≥n-y-fundamentos-te√≥ricos)
2. [Configuraci√≥n e Implementaci√≥n Pr√°ctica](#2-configuraci√≥n-e-implementaci√≥n-pr√°ctica)
3. [Arquitectura del Workflow n8n](#3-arquitectura-del-workflow-n8n)
4. [Persistencia e Integraci√≥n de Datos](#4-persistencia-e-integraci√≥n-de-datos)
5. [Despliegue y Mantenimiento](#5-despliegue-y-mantenimiento)

---

## 1. Introducci√≥n y Fundamentos Te√≥ricos

### 1.1 Visi√≥n General del Proyecto

La adquisici√≥n automatizada de datos inmobiliarios de Portal Inmobiliario representa un caso de uso avanzado de web scraping que trasciende la simple extracci√≥n de datos. Este proyecto requiere una metodolog√≠a profesional que combine inteligencia t√©cnica, respeto por las mejores pr√°cticas √©ticas y una arquitectura robusta dise√±ada para la producci√≥n.

### 1.2 An√°lisis T√©cnico de Portal Inmobiliario

Portal Inmobiliario es una plataforma web moderna que depende heavily de la renderizaci√≥n del lado del cliente. Las caracter√≠sticas t√©cnicas clave incluyen:

- **Arquitectura SPA**: Aplicaci√≥n de p√°gina √∫nica con carga din√°mica de contenido
- **Renderizaci√≥n JavaScript**: Los datos de propiedades se cargan din√°micamente post-renderizaci√≥n inicial
- **Sistema de Paginaci√≥n**: URLs con patr√≥n `/Desde_X` sugieren paginaci√≥n basada en offset
- **APIs Internas**: El front-end consume APIs JSON para poblar el contenido

#### Estrategia API-First

La metodolog√≠a m√°s robusta consiste en identificar las APIs internas que utiliza el front-end:

1. **Abrir DevTools** en Chrome (F12)
2. **Filtrar por XHR/Fetch** en la pesta√±a Network
3. **Navegar entre p√°ginas** para capturar las llamadas API
4. **Identificar endpoints** que devuelven datos JSON estructurados
5. **Copiar como cURL** para replicar en n8n

### 1.3 Marco √âtico y Legal

- **T√©rminos de Servicio**: Portal Inmobiliario generalmente proh√≠be extracci√≥n automatizada
- **robots.txt**: El archivo no es v√°lido/accesible, creando zona gris
- **Best Practices**: Implementar rate limiting agresivo y usar User-Agent descriptivo
- **Responsabilidad**: Operar de manera conservadora para evitar interrupciones

---

## 2. Configuraci√≥n e Implementaci√≥n Pr√°ctica

### 2.1 Requisitos Previos

- ‚úÖ n8n configurado y funcionando (http://n8n.gabrielpantoja.cl)
- ‚úÖ PostgreSQL disponible para almacenamiento
- ‚úÖ Cuenta con servicio de scraping (HTTP Request o servicio especializado)
- ‚úÖ Gmail configurado para notificaciones

### 2.2 Configuraci√≥n de Base de Datos

Ejecutar el script de configuraci√≥n de BD:

```bash
# Desde el directorio ra√≠z del proyecto
./scripts/setup-db.sh
```

Este script:
- Verifica que PostgreSQL est√© ejecut√°ndose
- Crea las tablas `properties` y `error_logs`
- Configura √≠ndices y vistas para performance
- Ejecuta verificaciones de integridad

#### Esquema de Base de Datos

```sql
-- Tabla principal de propiedades
CREATE TABLE properties (
    id VARCHAR(50) PRIMARY KEY,
    title TEXT,
    price DECIMAL(15,2),
    currency VARCHAR(10),
    property_type VARCHAR(50),
    bedrooms INTEGER,
    bathrooms INTEGER,
    surface_area DECIMAL(10,2),
    location TEXT,
    city VARCHAR(100),
    region VARCHAR(100),
    description TEXT,
    url TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabla de logs de errores
CREATE TABLE error_logs (
    id SERIAL PRIMARY KEY,
    workflow_name VARCHAR(100),
    error_message TEXT,
    error_details JSONB,
    occurred_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 2.3 Configuraci√≥n de Credenciales en n8n

#### 2.3.1 Credenciales PostgreSQL
1. Ir a **Settings** ‚Üí **Credentials** en n8n
2. A√±adir credencial **PostgreSQL**
3. Configurar:
   - **Name**: `PostgreSQL` o `postgres-main`
   - **Host**: `localhost` (o IP del servidor)
   - **Database**: `postgres`
   - **User**: `postgres`
   - **Password**: [tu password de PostgreSQL]
   - **Port**: `5432`

#### 2.3.2 Credenciales Gmail (para alertas)
1. A√±adir credencial **Gmail OAuth2**
2. Seguir el proceso de autenticaci√≥n OAuth
3. **Name**: `Gmail` o `gmail-main`

#### 2.3.3 HTTP Request Directo (Recomendado)
**No requiere credenciales externas**. Configuraci√≥n b√°sica con headers est√°ndar.

---

## 3. Arquitectura del Workflow n8n

### 3.1 Componentes del Workflow Principal

#### 3.1.1 Trigger y Control de Flujo
```
Manual Trigger (desarrollo) ‚Üí Cron Trigger (producci√≥n)
    ‚Üì
Initialize Variables (Edit Fields)
    ‚Üì
Loop Control (Item Lists)
```

#### 3.1.2 Motor de Extracci√≥n
```
HTTP Request Node (API call)
    ‚Üì
Item Lists (Split Out Items)
    ‚Üì
Edit Fields (Data transformation)
    ‚Üì
Code Node (Advanced processing)
```

#### 3.1.3 Persistencia y Alertas
```
PostgreSQL Insert/Update
    ‚Üì
Error Handling Branch
    ‚Üì
Gmail Notification (on errors)
```

### 3.2 Configuraci√≥n del HTTP Request Node

#### HTTP Request Directo (Recomendado)
```json
{
  "method": "GET",
  "url": "={{ $json.baseUrl }}/_Desde_{{ $json.offset }}",
  "sendHeaders": true,
  "headerParameters": {
    "parameters": [
      {
        "name": "User-Agent",
        "value": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
      },
      {
        "name": "Accept",
        "value": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
      },
      {
        "name": "Accept-Language",
        "value": "es-CL,es;q=0.8,en;q=0.6"
      }
    ]
  },
  "options": {
    "timeout": 60000,
    "retry": {
      "enabled": true,
      "maxTries": 3,
      "waitBetween": 2000
    },
    "batching": {
      "enabled": false
    }
  }
}
```

#### Para Servicio de Scraping (Opcional)
```json
{
  "method": "POST",
  "url": "https://api.scrapeninja.net/scrape",
  "headers": {
    "X-RapidAPI-Key": "{{ $credentials.scrapeninja.apikey }}"
  },
  "body": {
    "url": "https://www.portalinmobiliario.com/venta/valdivia-los-rios/_Desde_{{ $json.offset }}",
    "retryNum": 3,
    "geo": "CL",
    "renderJs": true
  }
}
```

### 3.3 L√≥gica de Paginaci√≥n

```javascript
// En Edit Fields - Inicializaci√≥n
{
  "offset": 0,
  "hasMorePages": true,
  "pageSize": 50
}

// En Code Node - Incremento de p√°gina
const currentOffset = items[0].json.offset;
const pageSize = items[0].json.pageSize;
const resultCount = items[0].json.results.length;

return [{
  json: {
    offset: currentOffset + pageSize,
    hasMorePages: resultCount === pageSize,
    pageSize: pageSize
  }
}];
```

### 3.4 Transformaci√≥n de Datos

```javascript
// Code Node - Limpieza y estructuraci√≥n
const properties = [];

for (const item of items) {
  const rawData = item.json;

  properties.push({
    id: rawData.id || `${rawData.title}_${Date.now()}`,
    title: rawData.title?.trim(),
    price: parseFloat(rawData.price?.toString().replace(/[^\d.]/g, '')),
    currency: rawData.currency || 'CLP',
    property_type: rawData.type || 'unknown',
    bedrooms: parseInt(rawData.bedrooms) || null,
    bathrooms: parseInt(rawData.bathrooms) || null,
    surface_area: parseFloat(rawData.surface) || null,
    location: rawData.location?.trim(),
    city: extractCity(rawData.location),
    region: extractRegion(rawData.location),
    description: rawData.description?.trim(),
    url: rawData.url,
    scraped_at: new Date().toISOString()
  });
}

function extractCity(location) {
  // L√≥gica para extraer ciudad
  return location?.split(',')[0]?.trim();
}

function extractRegion(location) {
  // L√≥gica para extraer regi√≥n
  return location?.split(',').pop()?.trim();
}

return properties.map(prop => ({ json: prop }));
```

---

## 4. Persistencia e Integraci√≥n de Datos

### 4.1 Operaciones de Base de Datos

#### Insert con Upsert Logic
```sql
-- PostgreSQL Node Query
INSERT INTO properties (
  id, title, price, currency, property_type,
  bedrooms, bathrooms, surface_area, location,
  city, region, description, url, scraped_at
) VALUES (
  $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14
)
ON CONFLICT (id) DO UPDATE SET
  price = EXCLUDED.price,
  title = EXCLUDED.title,
  scraped_at = EXCLUDED.scraped_at
RETURNING id;
```

#### Par√°metros del Query
```javascript
[
  "{{ $json.id }}",
  "{{ $json.title }}",
  "{{ $json.price }}",
  "{{ $json.currency }}",
  "{{ $json.property_type }}",
  "{{ $json.bedrooms }}",
  "{{ $json.bathrooms }}",
  "{{ $json.surface_area }}",
  "{{ $json.location }}",
  "{{ $json.city }}",
  "{{ $json.region }}",
  "{{ $json.description }}",
  "{{ $json.url }}",
  "{{ $json.scraped_at }}"
]
```

### 4.2 Manejo de Errores y Logging

#### Query SQL Simplificada (Sin error_details)
```sql
-- Insert Error Log (Configuraci√≥n actual)
INSERT INTO error_logs (workflow_name, error_message, occurred_at)
VALUES ($1, $2, CURRENT_TIMESTAMP)
RETURNING id;
```

#### Configuraci√≥n en n8n
**Query Parameters:** `={{ [$json.workflow_name, $json.error_message] }}`

#### Si tu tabla tiene error_details:
```sql
-- Insert Error Log (Con error_details)
INSERT INTO error_logs (workflow_name, error_message, error_details, occurred_at)
VALUES ($1, $2, $3, CURRENT_TIMESTAMP)
RETURNING id;
```

#### Code Node - Error Processing
```javascript
// Preparar datos de error
const errorDetails = {
  error: error.message,
  item: item.json,
  timestamp: new Date().toISOString(),
  stack: error.stack
};

return [{
  json: {
    workflow_name: 'Portal Inmobiliario Scraper',
    error_message: error.message,
    error_details: JSON.stringify(errorDetails) // Solo si tu tabla lo soporta
  }
}];
```

### 4.3 Configuraci√≥n de Rate Limiting

En el HTTP Request Node:
- **Continue on Fail**: ‚úÖ Habilitado
- **Retry on Fail**: ‚úÖ Habilitado (3 intentos)
- **Batching**: Items per Batch = 1, Batch Interval = 3000ms

---

## 5. Despliegue y Mantenimiento

### 5.1 Programaci√≥n Autom√°tica

#### Cron Trigger Configuration
```
# Ejecutar diariamente a las 3:00 AM (hora local)
0 3 * * *

# Ejecutar cada 6 horas
0 */6 * * *

# Ejecutar solo d√≠as laborales a las 9:00 AM
0 9 * * 1-5
```

### 5.2 Monitoreo y Alertas

#### Email Notification Node
```json
{
  "fromEmail": "sistema@tudominio.com",
  "toEmail": "admin@tudominio.com",
  "subject": "üö® Error en Portal Inmobiliario Scraper",
  "emailFormat": "html",
  "message": `
    <h2>Error en Workflow</h2>
    <p><strong>Workflow:</strong> {{ $json.workflow_name }}</p>
    <p><strong>Error:</strong> {{ $json.error_message }}</p>
    <p><strong>Tiempo:</strong> {{ $now }}</p>
    <p><strong>Detalles:</strong></p>
    <pre>{{ $json.error_details }}</pre>
  `
}
```

### 5.3 Consultas de An√°lisis

#### Estad√≠sticas de Propiedades
```sql
-- Propiedades por tipo
SELECT
    property_type,
    COUNT(*) as count,
    AVG(price) as avg_price,
    MIN(price) as min_price,
    MAX(price) as max_price
FROM properties
WHERE price IS NOT NULL
GROUP BY property_type
ORDER BY count DESC;

-- Trending por ubicaci√≥n (√∫ltimos 7 d√≠as)
SELECT
    city,
    COUNT(*) as listings,
    AVG(price) as avg_price
FROM properties
WHERE scraped_at >= CURRENT_DATE - INTERVAL '7 days'
    AND city IS NOT NULL
GROUP BY city
ORDER BY listings DESC
LIMIT 10;

-- Evoluci√≥n de precios
SELECT
    DATE(scraped_at) as date,
    COUNT(*) as properties_scraped,
    AVG(price) as avg_price
FROM properties
WHERE scraped_at >= CURRENT_DATE - INTERVAL '30 days'
    AND price IS NOT NULL
GROUP BY DATE(scraped_at)
ORDER BY date;
```

#### Estad√≠sticas de Valdivia (Espec√≠ficas)
```sql
-- Vista espec√≠fica para Valdivia
CREATE VIEW valdivia_property_stats AS
SELECT
    property_type,
    COUNT(*) as total_listings,
    AVG(price) as avg_price,
    AVG(surface_area) as avg_surface,
    AVG(bedrooms) as avg_bedrooms
FROM properties
WHERE city ILIKE '%valdivia%'
    AND region ILIKE '%los r√≠os%'
GROUP BY property_type;

-- Propiedades recientes en Valdivia
CREATE VIEW recent_valdivia_properties AS
SELECT *
FROM properties
WHERE city ILIKE '%valdivia%'
    AND scraped_at >= CURRENT_DATE - INTERVAL '7 days'
ORDER BY scraped_at DESC;
```

### 5.4 Mantenimiento y Optimizaci√≥n

#### Limpieza Peri√≥dica
```sql
-- Funci√≥n para limpiar propiedades antiguas
CREATE OR REPLACE FUNCTION clean_old_properties(days_old INTEGER DEFAULT 90)
RETURNS INTEGER AS $$
DECLARE
    deleted_count INTEGER;
BEGIN
    DELETE FROM properties
    WHERE scraped_at < CURRENT_DATE - INTERVAL days_old || ' days';

    GET DIAGNOSTICS deleted_count = ROW_COUNT;

    INSERT INTO error_logs (workflow_name, error_message, error_details)
    VALUES ('Maintenance', 'Cleanup completed',
            json_build_object('deleted_records', deleted_count, 'days_old', days_old));

    RETURN deleted_count;
END;
$$ LANGUAGE plpgsql;
```

#### √çndices para Performance
```sql
-- √çndices recomendados
CREATE INDEX IF NOT EXISTS idx_properties_city ON properties(city);
CREATE INDEX IF NOT EXISTS idx_properties_scraped_at ON properties(scraped_at);
CREATE INDEX IF NOT EXISTS idx_properties_price ON properties(price) WHERE price IS NOT NULL;
CREATE INDEX IF NOT EXISTS idx_properties_type ON properties(property_type);
```

### 5.5 Troubleshooting Common Issues

#### 1. Error "batchInterval" en HTTP Request
**Problema:** `Cannot read properties of undefined (reading 'batchInterval')`
**Soluci√≥n:**
- Cambiar `batching` a `"enabled": false`
- Usar `typeVersion: 4.1` en lugar de `4.2`
- Eliminar propiedades `batchSize` y `batchInterval`

#### 2. Error "column does not exist" en PostgreSQL
**Problema:** `column "error_details" of relation "error_logs" does not exist`
**Soluci√≥n:**
- **Query simplificada:** `INSERT INTO error_logs (workflow_name, error_message, occurred_at) VALUES ($1, $2, CURRENT_TIMESTAMP)`
- **Query Parameters:** `={{ [$json.workflow_name, $json.error_message] }}`
- O crear la columna: `ALTER TABLE error_logs ADD COLUMN error_details JSONB;`

#### 3. Error "there is no parameter $1"
**Problema:** Faltan Query Parameters en nodo PostgreSQL
**Soluci√≥n:**
- En **Additional Fields** ‚Üí activar **Query Parameters**
- Configurar: `={{ [$json.workflow_name, $json.error_message] }}`
- O usar query sin par√°metros: `VALUES ('{{ $json.workflow_name }}', '{{ $json.error_message }}', CURRENT_TIMESTAMP)`

#### 4. HTTP Request sin credenciales
**Problema:** Requests fallando por falta de API keys
**Soluci√≥n:**
- Usar m√©todo **GET** directo a Portal Inmobiliario
- Configurar User-Agent y headers b√°sicos
- Evitar servicios externos como ScrapeNinja

#### 5. Actor no encuentra propiedades
- Verificar que Portal Inmobiliario no haya cambiado estructura
- Revisar rate limiting
- Actualizar User-Agent

#### 6. Errores de base de datos
- Verificar credenciales PostgreSQL
- Comprobar que las tablas existan
- Revisar permisos de usuario

#### 7. Notificaciones no llegan
- Verificar credenciales Gmail
- Revisar spam/filtros
- Comprobar permisos OAuth

### 5.6 Estrategias de Resiliencia

#### M√∫ltiples Fuentes de Datos
- Configurar m√∫ltiples endpoints como fallback
- Implementar rotaci√≥n de User-Agents
- Usar proxies si es necesario

#### Backup de Configuraci√≥n
```bash
# Exportar workflow
curl -X GET "http://localhost:5678/api/v1/workflows/export" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  > portal-inmobiliario-workflow-backup.json
```

---

## üéØ Conclusiones Estrat√©gicas

### Mejores Pr√°cticas Implementadas

1. **API-First Approach**: Priorizar intercepci√≥n de APIs sobre scraping HTML
2. **Arquitectura Resiliente**: Manejo robusto de errores y reintentos
3. **Rate Limiting Responsable**: Respeto por los recursos del servidor objetivo
4. **Monitoreo Proactivo**: Alertas autom√°ticas y logging detallado
5. **Mantenimiento Planificado**: Estrategias de limpieza y optimizaci√≥n

### Expansi√≥n Futura

- **M√∫ltiples Portales**: Yapo.cl, MercadoLibre Inmuebles
- **IA y ML**: An√°lisis de sentimientos, predicci√≥n de precios
- **Dashboard**: Visualizaciones en tiempo real
- **APIs P√∫blicas**: Exposici√≥n de datos estructurados

### Consideraciones √âticas

- Cumplimiento de t√©rminos de servicio
- Rate limiting conservador
- Uso responsable de recursos
- Transparencia en User-Agent
- Respeto por medidas anti-bot

---

**¬øNecesitas ayuda con alg√∫n paso espec√≠fico?** Esta gu√≠a proporciona una base s√≥lida para implementar un sistema profesional de scraping inmobiliario con n8n. üöÄ